# -*- coding: utf-8 -*-
"""NODE/POD FAILURE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10YWHwC2RKDv7r3WVQ0k4y5oEeiF7B1rt

**NODE/ POD FAILURE PRECICTION USING RANDOM FOREST CLASSIFIER AND XGBOOST**
"""

from google.colab import files
import pandas as pd
import xgboost as xgb
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
from sklearn.metrics import accuracy_score, classification_report

"""STEP 1 : LOAD THE DATASET"""

import pandas as pd

# Load the dataset
df = pd.read_csv("/content/k8s_system_failures.csv")

# Display dataset info
print("Dataset Overview:")
print(df.info())
print("\nFirst 5 rows:")
print(df.head())
# Check for missing values
print("Missing Values:\n", df.isnull().sum())

# Drop missing values (if necessary)
df.dropna(inplace=True)  # OR, fill missing values with a default

"""STEP 2 : Train-Test Split"""

# Convert Percentage Strings to Float
for col in ["CPU_Usage", "Memory_Usage", "Disk_Usage"]:
    df[col] = df[col].str.rstrip('%').astype(float)

# Encode Categorical Column 'Network_Usage'
le = LabelEncoder()
df["Network_Usage"] = le.fit_transform(df["Network_Usage"])

# Define Features (X) and Target (y)
X = df[["CPU_Usage", "Memory_Usage", "Disk_Usage", "Network_Usage"]]
y = df["Pod_Status"].apply(lambda x: 1 if x == "CrashLoopBackOff" else 0)

# Apply SMOTE for Class Balancing
sm = SMOTE(random_state=42)
X_resampled, y_resampled = sm.fit_resample(X, y)

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled
)

"""STEP 3 : Train Random Forest Model and XGBoost Model"""

# Train Random Forest Model
rf_model = RandomForestClassifier(n_estimators=40, max_depth=6, random_state=42)
rf_model.fit(X_train, y_train)

#Train XGBoost Model
xgb_model = xgb.XGBClassifier(
    n_estimators=500,  # Increase trees (better learning)
    max_depth=8,  # Deeper trees capture more patterns
    learning_rate=0.05,  # Lower learning rate improves generalization
    subsample=0.8,  # Prevents overfitting
    colsample_bytree=0.95,  # Uses only 80% of features per tree (better variety)
    reg_lambda=1,  # L2 regularization (reduces overfitting)
    random_state=42
)
xgb_model.fit(X_train, y_train)

"""STEP 4 : Evaluate Both Models"""

# Evaluate Both Models
def evaluate_model(model, X_test, y_test, model_name, threshold=0.6):
    y_pred_probs = model.predict_proba(X_test)[:, 1]  # Get probability for Class 1 (Failure)
    y_pred_adjusted = (y_pred_probs > threshold).astype(int)

    accuracy = accuracy_score(y_test, y_pred_adjusted)
    print(f"✅ {model_name} Model Accuracy: {accuracy:.2f}")
    print(f"Classification Report for {model_name}:\n", classification_report(y_test, y_pred_adjusted))

"""STEP 5 : Comparing Both Models"""

# Compare Both Models
evaluate_model(rf_model, X_test, y_test, "Random Forest")
evaluate_model(xgb_model, X_test, y_test, "XGBoost")

"""STEP 6 : Plots Analysis

"""

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

#  Plot Confusion Matrix
def plot_confusion_matrix(y_true, y_pred, model_name):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(5,4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["No Failure", "Failure"], yticklabels=["No Failure", "Failure"])
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title(f"{model_name} - Confusion Matrix")
    plt.show()

# Predictions for Both Models
rf_y_pred_probs = rf_model.predict_proba(X_test)[:, 1]
rf_y_pred = (rf_y_pred_probs > 0.5).astype(int)

xgb_y_pred_probs = xgb_model.predict_proba(X_test)[:, 1]
xgb_y_pred = (xgb_y_pred_probs > 0.5).astype(int)

# ✅ Plot Confusion Matrices
plot_confusion_matrix(y_test, rf_y_pred, "Random Forest")
plot_confusion_matrix(y_test, xgb_y_pred, "XGBoost")

from sklearn.metrics import roc_curve, auc

# Plot ROC Curve
def plot_roc_curve(y_true, y_probs, model_name):
    fpr, tpr, _ = roc_curve(y_true, y_probs)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(6,5))
    plt.plot(fpr, tpr, color="blue", lw=2, label=f"AUC = {roc_auc:.2f}")
    plt.plot([0, 1], [0, 1], color="grey", linestyle="--")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title(f"{model_name} - ROC Curve")
    plt.legend(loc="lower right")
    plt.show()

# Plot for both models
plot_roc_curve(y_test, rf_y_pred_probs, "Random Forest")
plot_roc_curve(y_test, xgb_y_pred_probs, "XGBoost")

from sklearn.metrics import precision_recall_curve

# Plot Precision-Recall Curve
def plot_precision_recall(y_true, y_probs, model_name):
    precision, recall, _ = precision_recall_curve(y_true, y_probs)

    plt.figure(figsize=(6,5))
    plt.plot(recall, precision, marker='.', label=f"{model_name}")
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.title(f"{model_name} - Precision-Recall Curve")
    plt.legend()
    plt.show()

# Plot for Both Models
plot_precision_recall(y_test, rf_y_pred_probs, "Random Forest")
plot_precision_recall(y_test, xgb_y_pred_probs, "XGBoost")

#  Compare True Labels & Predicted Labels
errors_rf = y_test - rf_y_pred
errors_xgb = y_test - xgb_y_pred

plt.figure(figsize=(6,5))
plt.hist(errors_rf, bins=3, alpha=0.6, label="Random Forest", color="blue")
plt.hist(errors_xgb, bins=3, alpha=0.6, label="XGBoost", color="red")
plt.xticks([-1, 0, 1], ["False Negative", "Correct", "False Positive"])
plt.xlabel("Prediction Error")
plt.ylabel("Count")
plt.legend()
plt.title("Error Distribution: Random Forest vs. XGBoost")
plt.show()